есть смысл пробовать +- weight_decay в районе 0.01-0.001
есть смысл пробовать ослаблять drop rate (когда у тебя батчнорм и, опционально, weigh_decay)
разумно двигать lr в районе 0.1-0.005
двойной batchnorm словно усредняет результаты между train/test (а ещё делает остатки более нормальными), 
		а одинарный -- повышает переобучение, значительно повышая качество в train


на TS большим нейронкам без weight_decay вообще никуда -- 
он защищает от инверсии лоссов и обеспечивает стабильность предиктов

те TS, что склоняются к lag1, часто "корректируют" его -- они сглаживают резкие значения --
похоже, так чтобы соответствующий loss от резкого скачка сначала на y_true, а потом на y_hat был меньше, был сглажен

гипотеза с лучшим 1х батчнормом на плохо предсказываемых задачах находит контрпримеры 
на выборке с экстремальными выбросами

wd может быть причиной схлопывания модели до тупого "завтра будет как вчера" -- по понятным причинам
эффект "завтра будет как вчера" наблюдается в двух видах
при первом он проявляется крайне жёстко -- исключительно "как вчера", без отклонений
второй вид характерен для случаев, когда модельке не хватило сил достать закономерность, и она берёт как вчера, но
немного сглаживает экстремальные значения в залагиваниях, чтобы понизить лоссы

"завтра будет как вчера" проявляется не обязательно сопутствующе wd>0, наблюдается и без него (полагаю, 
это эффект регуляризации батчнормами может играть роль, либо дропрейты, хотя возможно и что-то иное)

Mish показал себя странно, словно он на одной выборке исправил lag1 к разумному, хоть и немного менее точному прогнозу

предположение: когда информации в иксах нет, модель имеет тенденцию склоняться к следующей дилемме:
либо переобучаться на трейне (при снижении регуляризации), либо выбирать lag1 от таргета с шумом (при повышении регуляризации)

я бы не списывал каттеры/стандардайзеры, просто они опциональны "на после"

мне кажется, или на некоторых датасетах модификации из специального блока не дают вообще никаких улучшений?
словно работают лучше всего некоторые "базовые" спецификации

Linear и GRU имеют тенденцию к "взрывам" на временных рядах, в то время как у LSTM такого не наблюдается

подверждается: про standardscale, про батчнормы -- их очень рекомендуется юзать
               weigh_decay / отсутствие; снижение droprate при батчнормах; pca / kendallsig; -- опционально


может, считать, что если 2x batchnorm результат даёт хуже, чем 1x, то конфигурация 
в принципе не очень? потому что слишком часто такая ситуация на совсем плохом перформансе

2х батчнорм требует в среднем более высокие lr для вменяемых результатов, чем 1х?


малая нейронка иногда может вообще плохо работать
часто вижу уже, что лучше результат на lr 0.1, а начиная с 0.01 результаты становятся плохие

хотя 2х батчнорм показывает часто результаты лучше, чем 1х, он также более склонен к переобучению,
если есть предпосылки к таковому

L1Loss как вариант я бы оставлял, он иногда неплох